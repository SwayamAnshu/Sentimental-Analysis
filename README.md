# Sentiment Analysis using Deep Learning (SNN, CNN, LSTM with GloVe Embeddings)

1. This project implements a sentiment analysis system on a labeled dataset of tweets. The models used include:
- Simple Neural Network (SNN)
- Convolutional Neural Network (CNN)
- Long Short-Term Memory (LSTM)

We utilize pre-trained GloVe word embeddings for word representation and analyze the model performance using metrics like accuracy, loss curves, confusion matrix, and classification reports.

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ data/

â”‚ â””â”€â”€ file.csv # Labeled tweet dataset

â”œâ”€â”€ embeddings/
â”‚ â””â”€â”€ glove.6B.100d.txt # Pretrained GloVe vectors

â”œâ”€â”€ embedding_matrix.csv # Extracted embedding matrix for model training

â”œâ”€â”€ sentiment_analysis.py # Main training and evaluation script

â”œâ”€â”€ requirements.txt # Required Python packages

â””â”€â”€ README.md # Project documentation


---

## ğŸ§  Model Architectures

- **SNN (Simple Neural Network)**: Embedding â†’ Flatten â†’ Dense â†’ Dropout â†’ Output
- **CNN**: Embedding â†’ Conv1D â†’ GlobalMaxPooling1D â†’ Dense â†’ Dropout â†’ Output
- **LSTM**: Embedding â†’ LSTM â†’ Dense â†’ Dropout â†’ Output

---

## ğŸ“Š Dataset

- The dataset `file.csv` contains two columns:
  - `tweets`: Text data.
  - `labels`: One of `good`, `neutral`, or `bad`.

Label encoding:
- `good` â†’ 1
- `neutral` â†’ 2
- `bad` â†’ 0

---

## ğŸ”§ Preprocessing Steps

- Lowercasing text
- Removing HTML tags, URLs, punctuation, and stopwords
- Tokenization and padding to fixed length (100)
- Word embeddings using 100-dimensional GloVe vectors

---

## ğŸš€ Getting Started

### 1. Clone the repository

git clone https://github.com/yourusername/sentiment-analysis-glove.git
cd sentiment-analysis-glove

2. Install dependencies
   pip install -r requirements.txt

3. Add Data and Embeddings
   -> Place your file.csv under the data/ directory.
   -> Download GloVe embeddings and place glove.6B.100d.txt under embeddings/.
4. Run the script
   python sentiment_analysis.py
   
ğŸ“ˆ Results
Each model's performance is evaluated using:

Accuracy and Loss curves (training vs validation)

Confusion Matrix

Precision, Recall, and F1-score (classification report)

ğŸ“Œ Notes
GloVe vectors are loaded from glove.6B.100d.txt

The model saves the best weights using ModelCheckpoint during training

ğŸ“œ License
This project is licensed under the MIT License.

ğŸ”„ Project Workflow Diagram -->

Raw Tweets (CSV)
      â”‚
      â–¼
[ Preprocessing ]
   - Clean text
   - Remove stopwords
   - Tokenize & Pad
   - Label Encoding
      â”‚
      â–¼
[ GloVe Embedding Matrix ]
      â”‚
      â–¼
[ Choose Model ]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    SNN     â”‚    CNN     â”‚   LSTM     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
[ Model Training & Validation ]
      â”‚
      â–¼
[ Evaluation ]
   - Accuracy & Loss Graphs
   - Confusion Matrix
   - Classification Report


ğŸ§   Model Architecture Diagrams
1. Simple Neural Network (SNN)

  Input (100 tokens)
      â”‚
[ Embedding Layer (GloVe) ]
      â”‚
[ Flatten ]
      â”‚
[ Dense (128) + ReLU ]
      â”‚
[ Dropout (0.5) ]
      â”‚
[ Dense (3) + Softmax ]


2. Convolutional Neural Network (CNN)

  Input (100 tokens)
      â”‚
[ Embedding Layer (GloVe) ]
      â”‚
[ Conv1D (128 filters, kernel=5) ]
      â”‚
[ GlobalMaxPooling1D ]
      â”‚
[ Dense (128) + ReLU ]
      â”‚
[ Dropout (0.5) ]
      â”‚
[ Dense (3) + Softmax ]


3. Long Short-Term Memory (LSTM)

  Input (100 tokens)
      â”‚
[ Embedding Layer (GloVe) ]
      â”‚
[ LSTM (128 units) ]
      â”‚
[ Dense (128) + ReLU ]
      â”‚
[ Dropout (0.5) ]
      â”‚
[ Dense (3) + Softmax ]

